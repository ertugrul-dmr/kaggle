{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nWhen I started doing this analysis my main goal was getting experience. I'm still learning and trying to improve my skills, so there might be some areas can be improved. My main objectives on this project are:\n\n*  **Explorating and visualising the data, trying to get some insights about our dataset**\n*  **Getting data in better shape by feature engineering to help us in building better models**\n*  **Building and tuning couple regression models to get some stable results on predicting house prices**\n\n## Some Notes\n\n* There are some kernels inspired me to follow this path on my work, you can find the links in related parts of the code if you are interested.\n* Please let me know if you have any recommendations or improvements, you can leave a comment at the end of this notebook.\n* I appreciate any feedback since I'm trying to learn more! Please feel free to tell me how can I improve...\n* And if you liked this one please don't forget to upvote!\n\n### This is an early version of my notebook, I'll try to improve it whenever I can, there might be parts missings markdowns, explanations etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load neccesary packages\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nfrom scipy import stats\nfrom scipy.stats import skew, boxcox_normmax, norm\nfrom scipy.special import boxcox1p\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, Normalizer\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load and inspect train-test datas\ntrain = pd.read_csv('/kaggle/input/home-data-for-ml-course/train.csv')\ntest = pd.read_csv('/kaggle/input/home-data-for-ml-course/test.csv')\nprint('Train set size:', train.shape)\nprint('Test set size:', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train.sample(5))\ndisplay(test.sample(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping not needed columns from both datasets\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# First Inspection"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Display numerical correlations between features on heatmap\nsns.set(font_scale=1.1)\ncorrelation_train = train.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(30, 20))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Listing most related numerical features to target.\ntrain_corr = train.corr().abs().unstack().sort_values(\n    kind='quicksort', ascending=False).reset_index()\ntrain_corr.rename(columns={\n    'level_0': 'Feature A',\n    'level_1': 'Feature B',\n    0: 'Correlation Coefficient'\n},\n    inplace=True)\ntrain_corr[(train_corr['Feature A'] == 'SalePrice') & (\n    train_corr['Correlation Coefficient'] >= 0.5)].style.background_gradient(\n        cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Backing up target variables and dropping them from train data\ny = train.SalePrice.reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Showing empirical target data set vs theororetical set\nfig = plt.figure(figsize=(6,6))\nax = fig.add_subplot(111)\nres = stats.probplot(train['SalePrice'], plot=plt)\n# Plotting the QQ_Plot. \nstats.probplot(train['SalePrice'], plot = plt)\nax.get_lines()[0].set_markerfacecolor('#e74c3c')\nax.get_lines()[0].set_markersize(12.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Merging train and test sets before engineering features for both.\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint(features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dealing with missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Barplot of missing values on both sets.\nfig, ax = plt.subplots(ncols=2, figsize=(20,6))\nmissing_tr = train.isnull().sum()\nmissing_te = test.isnull().sum()\nmissing_tr = missing_tr[missing_tr > 0]\nmissing_te = missing_te[missing_te > 0]\nmissing_tr.sort_values(ascending=False).plot(kind='bar', color='#e74c3c', ax=ax[0])\nmissing_te.sort_values(ascending=False).plot(kind='bar', color='#e74c3c', ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#List of NaN including columns where NaN's mean none.\nnone_cols = [\n    'Alley', 'PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'GarageType',\n    'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n    'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'\n]\n#List of NaN including columns where NaN's mean 0.\nzero_cols = [\n    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n    'BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea'\n]\n\n#List of NaN including columns where NaN's actually missing gonna replaced with mode.\nfreq_cols = [\n    'Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual',\n    'SaleType', 'Utilities'\n]\n#Filling the list of columns above:\nfor col in zero_cols:\n    features[col].replace(np.nan, 0, inplace=True)\n\nfor col in none_cols:\n    features[col].replace(np.nan, 'None', inplace=True)\n\nfor col in freq_cols:\n    features[col].replace(np.nan, features[col].mode()[0], inplace=True)\n\n#Filling MSZoning with using related MSSubClass and mode:    \nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(\n    lambda x: x.fillna(x.mode()[0]))\n\n#Features which numerical on data but should be treated as category.\nfeatures['MSSubClass'] = features['MSSubClass'].astype(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imputing missing data with SVR"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setting train and test sets for filling LotFrontage using SVR\nlot_train = features[features['LotFrontage'].notnull()]\nlot_test = features[features['LotFrontage'].isnull()]\n\n#Checking test and train values\nlot_target = lot_train['LotFrontage']\nprint(f'LotFrontage has {lot_test.shape[0]} missing value, and {lot_train.shape[0]} values available.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Basic function to spot outliers\ndef idOutliers(dat):\n    qtile25 = dat.quantile(q=0.25)\n    qtile75 = dat.quantile(q=0.75)\n    iqr = qtile75 - qtile25\n    out = (dat > qtile75 + 1.5 * iqr) | (dat < qtile25 - 1.5 * iqr)\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LotArea with outliers vs. LotArea without outliers\nfig, ax = plt.subplots(ncols=2, figsize=(16, 4))\nax[0].set_title('With LotArea outliers')\nax[1].set_title('Without LotArea outliers')\nsns.regplot(lot_train.LotArea.apply(np.sqrt), lot_target, ax=ax[0],color='#e74c3c')\nax[0].set(xlabel='sqrt(LotArea)')\nsns.regplot(lot_train.LotArea[~idOutliers(lot_train.LotArea)].apply(np.sqrt),\n            lot_target[~idOutliers(lot_train.LotArea)],\n            ax=ax[1],color='#e74c3c')\nax[1].set(xlabel='sqrt(LotArea)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Listing related numerical features of lot_train set\nlot_train_corr = lot_train.corr().abs().unstack().sort_values(\n    kind='quicksort', ascending=False).reset_index()\nlot_train_corr.rename(columns={\n    'level_0': 'Feature A',\n    'level_1': 'Feature B',\n    0: 'Correlation Coefficient'\n},\n    inplace=True)\nlot_train_corr[(lot_train_corr['Feature A'] == 'LotFrontage') & (\n    lot_train_corr['Correlation Coefficient'] >= 0.3)].style.background_gradient(\n        cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deciding target and train variables for svr model.\ny_lotFrontage = lot_train['LotFrontage']\nX_lot_train = lot_train.loc[:, [\n    'LotArea', 'LotConfig', 'LotShape', 'Alley', 'MSZoning', 'BldgType',\n    'Neighborhood', 'Condition1', 'Condition2', 'GarageCars', '1stFlrSF', 'GrLivArea', 'MSSubClass'\n]]\n\n#Getting dummy variables\nX_lot_train = pd.get_dummies(X_lot_train)\n#Normalization of train data\nX_lot_train = (X_lot_train - X_lot_train.mean()) / X_lot_train.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SVR(kernel='rbf', C=100, gamma=0.003)\n\nacc = 0\nacc1 = 0\nacc2 = 0\n\nkf = KFold(n_splits=10, shuffle=True, random_state=42)\n\nfor trn, tst in kf.split(lot_train):\n\n    # Compute benchmark score prediction based on mean neighbourhood LotFrontage\n    fold_train_samples = lot_train.iloc[trn]\n    fold_test_samples = lot_train.iloc[tst]\n    neigh_means = fold_train_samples.groupby(\n        'Neighborhood')['LotFrontage'].mean()\n    all_mean = fold_train_samples['LotFrontage'].mean()\n    y_pred_neigh_means = fold_test_samples.join(\n        neigh_means, on='Neighborhood', lsuffix='benchmark')['LotFrontage']\n    y_pred_all_mean = [all_mean] * fold_test_samples.shape[0]\n\n    # Compute benchmark score prediction based on overall mean LotFrontage\n    u1 = ((fold_test_samples['LotFrontage'] - y_pred_neigh_means)**2).sum()\n    u2 = ((fold_test_samples['LotFrontage'] - y_pred_all_mean)**2).sum()\n    v = ((fold_test_samples['LotFrontage'] -\n          fold_test_samples['LotFrontage'].mean())**2).sum()\n\n    # Perform model fitting\n    clf.fit(X_lot_train.iloc[trn], y_lotFrontage.iloc[trn])\n\n    # Record all scores for averaging\n    acc = acc + mean_absolute_error(fold_test_samples['LotFrontage'],\n                                    clf.predict(X_lot_train.iloc[tst]))\n    acc1 = acc1 + mean_absolute_error(fold_test_samples['LotFrontage'],\n                                      y_pred_neigh_means)\n    acc2 = acc2 + mean_absolute_error(fold_test_samples['LotFrontage'],\n                                      y_pred_all_mean)\n\nprint('10-Fold Validation Mean Absolute Error results:')\nprint('\\tSVR: {:.3}'.format(acc / 10))\nprint('\\tSingle mean: {:.3}'.format(acc2 / 10))\nprint('\\tNeighbourhood mean: {:.3}'.format(acc1 / 10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Looks like this version improved MAE results than basic filling methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_lot_test = lot_test.loc[:, [\n    'LotArea', 'LotConfig', 'LotShape', 'Alley', 'MSZoning', 'BldgType',\n    'Neighborhood', 'Condition1', 'Condition2', 'GarageCars', '1stFlrSF', 'GrLivArea', 'MSSubClass'\n]]\nX_lot_test = pd.get_dummies(X_lot_test)\nX_lot_test = (X_lot_test - X_lot_test.mean()) / X_lot_test.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make sure that dummy columns from training set are replicated in test set(for lot feature)\nfor col in (set(X_lot_train.columns) - set(X_lot_test.columns)):\n    X_lot_test[col] = 0\n\n\nX_lot_test = X_lot_test[X_lot_train.columns]\n\n# Assign predicted LotFrontage values back in features\nfeatures.loc[features.LotFrontage.isnull(), 'LotFrontage'] = clf.predict(\n    X_lot_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"My approach [based on this](https://www.kaggle.com/ogakulov/lotfrontage-fill-in-missing-values-house-prices) kernel. Wanted to thank him for the inspiration!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking if any missing values left\nprint(f'Missing values: {features.isna().sum().sum()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inspecting categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Listing categorical related to sale price\nobjts = [\n    'MSSubClass',\n    'MSZoning',\n    'LotConfig',\n    'Neighborhood',\n    'BldgType',\n    'HouseStyle',\n    'RoofStyle',\n    'KitchenQual',\n    'BsmtFinType1',\n    'BsmtFinType2',\n    'BsmtQual',\n    'BsmtCond',\n    'GarageType',\n    'GarageQual',\n    'GarageCond',\n]\n\n#Plotting violin plots to see relationships\n\nfig, axes =plt.subplots(5,3, figsize=(20,20))\naxes = axes.flatten()\n\nfor ax, catplot in zip(axes, objts):\n    sns.violinplot(x=catplot, y='SalePrice', data=train, ax=ax, palette='Spectral', inner='quartile')\n    ax.set_xticklabels(ax.get_xticklabels(),rotation=60)\n    ax.axhline(train.SalePrice.mean(),color='r',linestyle='--',linewidth=3)\n\nplt.tight_layout()  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating New Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating new features by merging closely related ones\nfeatures['YrBltAndRemod'] = features['YearBuilt'] + features['YearRemodAdd']\n\nfeatures['TotalSF'] = features['TotalBsmtSF'] + features[\n    '1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] +\n                                 features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n#Gave weights for half baths and basement half baths\nfeatures['Total_Bathrooms'] = (features['FullBath'] +\n                               (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] +\n                               (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] +\n                              features['ScreenPorch'] + features['WoodDeckSF'])\n\n#Merging quality and conditions\nfeatures['TotalQual'] = (features['OverallQual'] + features['OverallCond'])\n\n#Creating new features by using new quality feature\nfeatures['QualSF'] = features['TotalQual'] * features['TotalSF']\n\nfeatures['QlLivArea'] = (features['GrLivArea'] - features['LowQualFinSF'])/(1-features['TotalQual'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating new features by categorizing specific area metrics\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 'Y' if x > 0 else 'N')\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 'Y' if x > 0 else 'N')\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 'Y' if x > 0 else 'N')\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 'Y' if x > 0 else 'N')\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 'Y' if x > 0 else 'N')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting features after feature engineering because going to drop outliers from train data\ntrain = features.iloc[:len(y), :]\ntest = features.iloc[len(train):, :]\n\n#Backing up target variables\ntrain['SalePrice'] = y\n\n#Listing pearson corr's with revised features\ntrain_corr = train.corr().abs().unstack().sort_values(\n    kind='quicksort', ascending=False).reset_index()\ntrain_corr.rename(columns={\n    'level_0': 'Feature A',\n    'level_1': 'Feature B',\n    0: 'Correlation Coefficient'\n},\n    inplace=True)\ntrain_corr[(train_corr['Feature A'] == 'SalePrice') & (\n    train_corr['Correlation Coefficient'] >= 0.50)].style.background_gradient(\n        cmap='summer_r')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inspecting Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Handpicking higly related features with sale price to drop outliers.\nhigh_corr_cols = [\n    'OverallQual', 'TotalSF', 'GrLivArea','Total_sqr_footage', 'GarageCars', 'Total_Bathrooms',\n    'GarageArea', 'YrBltAndRemod', 'TotalBsmtSF', '1stFlrSF', 'FullBath', \n]\n\n\n\n#Checking for outliers in related features vs SalePrice\nfor i in high_corr_cols:\n    g = sns.JointGrid(x=i, y='SalePrice', data=train)\n    g = g.plot_joint(sns.regplot, color='#e74c3c', scatter_kws={'alpha': 0.4})\n    g = g.plot_marginals(sns.distplot,\n                         hist=True,\n                         kde=True,\n                         fit=norm,\n                         kde_kws={\n                             'shade': False,\n                             'legend': True\n                         },\n                         color='#e74c3c')\n    plt.xlabel(str(i))\n    plt.yticks(np.arange(0, 8000000, 50000))\n    g.fig.set_figwidth(12)\n    g.fig.set_figheight(12)\n    plt.style.use('fivethirtyeight')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping outliers after detecting them by eye\ntrain = train.drop(train[(train.OverallQual<5) & (train.SalePrice>200000)].index)\ntrain = train.drop(train[(train['TotalSF'] > 6000) & (train['SalePrice'] < 200000)].index)\ntrain = train.drop(train[(train['GrLivArea'] > 4000) & (train['SalePrice'] < 200000)].index)\ntrain = train.drop(train[(train.Total_Bathrooms>4) & (train.SalePrice<180000)].index)\ntrain = train.drop(train[(train.Total_Bathrooms<5) & (train.SalePrice>600000)].index)\ntrain = train.drop(train[(train.GarageArea>1200) & (train.SalePrice<100000)].index)\ntrain = train.drop(train[(train.GarageArea<1000) & (train.SalePrice>500000)].index)\ntrain = train.drop(train[(train.YrBltAndRemod<4000) & (train.SalePrice>450000)].index)\ntrain = train.drop(train[(train.TotalBsmtSF<1800) & (train.SalePrice>450000)].index)\ntrain = train.drop(train[(train.TotalBsmtSF>4000) & (train.SalePrice>200000)].index)\ntrain = train.drop(train[(train['1stFlrSF']<3000) & (train.SalePrice>600000)].index)\ntrain = train.drop(train[(train['1stFlrSF']>3000) & (train.SalePrice<200000)].index)\ntrain = train.drop(train[(train.FullBath<1) & (train.SalePrice>250000)].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Listing corr's after dropping outliers\ntrain_corr = train.corr().abs().unstack().sort_values(\n    kind='quicksort', ascending=False).reset_index()\ntrain_corr.rename(columns={\n    'level_0': 'Feature A',\n    'level_1': 'Feature B',\n    0: 'Correlation Coefficient'\n},\n    inplace=True)\ntrain_corr[(train_corr['Feature A'] == 'SalePrice') & (\n    train_corr['Correlation Coefficient'] >= 0.50)].style.background_gradient(\n        cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like our new features are highly related to our target values!"},{"metadata":{},"cell_type":"markdown","source":"# Processing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Log1p-transformation of the target variable\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n\n#Splitting test features after dropping some outliers\ntest_features = features.iloc[len(y):, :]\n\n#Backing up target variables and dropping them from train data\ny = train.SalePrice.reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\n\n#Merging features for the last time for transforming skewed features\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Showing empirical target data set vs theororetical set after transforming target variables\n\ndef plot_3chart(df, feature):\n    import matplotlib.gridspec as gridspec\n    plt.style.use('ggplot')\n    # Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    # creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    # Set the title. \n    ax1.set_title('Histogram')\n    # plot the histogram. \n    sns.distplot(df.loc[:,feature], hist=True,kde=True,fit=norm, ax = ax1,color='#e74c3c')\n    ax1.legend(labels=['Norm','Log'])\n    \n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    # Set the title. \n    ax2.set_title('Probability Plot')\n    # Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n    ax2.get_lines()[0].set_markerfacecolor('#e74c3c')\n    ax2.get_lines()[0].set_markersize(12.0)\n    \n\n    # Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title. \n    ax3.set_title('Violin Plot')\n    # Plotting the box plot. \n    sns.violinplot(df.loc[:,feature], orient='v', ax = ax3,color='#e74c3c');\n    \nplot_3chart(train, 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taking out list of numerical data:\nnumerics = [i for i in features.columns if features.dtypes[i] != 'object']\n\n#Finding skewness of the numerical features\nskew_features = features[numerics].apply(lambda x: skew(x)).sort_values(\n    ascending=False)\n\n#Filtering skewed features\nhigh_skew = skew_features[skew_features > 0.5]\n\n#Taking indexes of high skew\nskew_index = high_skew.index\n\n#Applying boxcox transformation to fix skewness\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))\n\n#Dropping irrevelant features\nfeatures = features.drop([\n    'Utilities',\n    'Street',\n    'PoolQC',\n], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting final features by one hot encoding\nprint(features.shape)\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\nprint(final_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting train and test data for the last time\nX_train = final_features.iloc[:len(y), :]\nX_test = final_features.iloc[len(X_train):, :]\n\nprint('X', X_train.shape, 'y', y.shape, 'X_test', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Detecting overfitting columns\noverfit = []\nfor i in X_train.columns:\n    counts = X_train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros / len(X_train) * 100 > 99.94:\n        overfit.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping overfitting columns\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n#\nX_train = X_train.drop(overfit, axis=1).copy()\nX_test = X_test.drop(overfit, axis=1).copy()\n#\nprint('X', X_train.shape, 'y', y.shape, 'X_test', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"nfolds = 10\nkf = KFold(nfolds, shuffle=True, random_state=42)\n\n\n# rmsle_cv\ndef rmsle_cv(model, trainx, trainy):\n    rmse = np.sqrt(np.abs(cross_val_score(model,\n                                          trainx.values,\n                                          trainy,\n                                          scoring='neg_mean_squared_error',\n                                          cv=kf,\n                                          n_jobs=-1)))\n    return (rmse)\n\n\n# rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n#r2 is optinal not added yet\ndef r2_cv(model, trainx, trainy):\n    rtw = cross_val_score(model,\n                          trainx.values,\n                          trainy,\n                          scoring='neg_mean_squared_error',\n                          cv=kf,\n                          n_jobs=-1)\n    return (rtw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas_alt = [27.5, 27.6, 27.7, 27.8, 27.9, 27, 27.1, 27.2, 27.3, 27.4, 27.5]\nalphas2 = [\n    5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008\n]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n\n\n#ridge_cv\nridge = make_pipeline(RobustScaler(), RidgeCV(\n    alphas=alphas_alt,\n    cv=kf,\n))\n\n#lasso_cv\nlasso = make_pipeline(\n    RobustScaler(),\n    LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kf))\n\n#elasticnet_cv\nelasticnet = make_pipeline(\n    RobustScaler(),\n    ElasticNetCV(max_iter=1e7,\n                 alphas=e_alphas,\n                 cv=kf,\n                 random_state=42,\n                 l1_ratio=e_l1ratio))\n\n#svr\nsvr = make_pipeline(RobustScaler(), SVR(\n    C=21,\n    epsilon=0.0099,\n    gamma=0.00017,\n    tol = 0.000121\n))\n\n#gradientboosting\ngbr = GradientBoostingRegressor(n_estimators=2900,\n                                learning_rate=0.0161,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=17,\n                                loss='huber',\n                                random_state=42)\n\n#lightgbm\nlightgbm = LGBMRegressor(\n    objective='regression',\n    n_estimators= 3500,\n    num_leaves= 5,\n    learning_rate= 0.00721,\n    max_bin= 163,\n    bagging_fraction= 0.35711,\n    n_jobs=-1,\n    bagging_seed=42,\n    feature_fraction_seed=42,\n    bagging_freq= 7,\n    feature_fraction= 0.1294,\n    min_data_in_leaf= 8\n    \n)\n\n\n\n#xgboost\nxgboost = XGBRegressor(learning_rate=0.0139,\n                       n_estimators=4500,\n                       max_depth=4,\n                       min_child_weight=0,\n                       subsample=0.7968,\n                       colsample_bytree=0.4064,\n                       nthread=-1,\n                       scale_pos_weight=2,\n                       seed=42,\n                          )\n\n#stack\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr,\n                                            xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)\n#models to crossval and fit\nmods = [ridge, lasso, elasticnet, svr, gbr, lightgbm, xgboost]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I tried to hyper-tune these models with optuna, that process is not included in this notebook."},{"metadata":{},"cell_type":"markdown","source":"# Model Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"mod_name = []\nrms = []\nrmstd = []\n\n#executing models\nfor i in mods:\n    score = rmsle_cv(i, X_train, y)\n    mod_name.append(i)\n    rms.append(score.mean())\n    rmstd.append(score.std())\n\n#creating a data frame out of crossval results\nmodel_df = pd.DataFrame(list(zip(mod_name, rms, rmstd)),\n                        columns=['Model', 'Rmsle', 'Rmsle Std'])\n\ndisplay(\n    model_df.sort_values(by='Rmsle', ascending=True).reset_index(\n        drop=True).style.background_gradient(cmap='summer'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks decent! I hope it didn't overfit..."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting models including stack\n\nprint('='*20,'START Fitting','='*20)\nprint('='*55)\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(np.array(X_train), np.array(y))\nprint(datetime.now(), 'Elasticnet')\nelastic_model_full_data = elasticnet.fit(X_train, y)\nprint(datetime.now(), 'Lasso')\nlasso_model_full_data = lasso.fit(X_train, y)\nprint(datetime.now(), 'Ridge')\nridge_model_full_data = ridge.fit(X_train, y)\nprint(datetime.now(), 'SVR')\nsvr_model_full_data = svr.fit(X_train, y)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X_train, y)\nprint(datetime.now(), 'XGboost')\nxgb_model_full_data = xgboost.fit(X_train, y)\nprint(datetime.now(), 'Lightgbm')\nlgb_model_full_data = lightgbm.fit(X_train, y)\nprint('='*20,'FINISHED Fitting','='*20)\nprint('='*58)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Blending"},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining weights for the models\n\ndef blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) +\n            (0.05 * lasso_model_full_data.predict(X)) +\n            (0.1 * ridge_model_full_data.predict(X)) +\n            (0.1 * svr_model_full_data.predict(X)) +\n            (0.1 * gbr_model_full_data.predict(X)) +\n            (0.15 * xgb_model_full_data.predict(X)) +\n            (0.1 * lgb_model_full_data.predict(X)) +\n            (0.3 * stack_gen_model.predict(np.array(X))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This blending approach was based on [this](https://www.kaggle.com/itslek/blend-stack-lr-gb-0-10649-house-prices-v57?scriptVersionId=11907006) kernel. I learned a lot from it. Thank you!"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\n    'Predict submission',\n    datetime.now(),\n)\n\n#Reading test data to add model predictions\nsubmission = pd.read_csv('/kaggle/input/home-data-for-ml-course/test.csv')\n#Inversing and flooring log scaled sale price predictions\nsubmission['SalePrice'] = np.floor(np.expm1(blend_models_predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining outlier quartile ranges\nq1 = submission['SalePrice'].quantile(0.0050)\nq2 = submission['SalePrice'].quantile(0.99)\n\n#applying weights to outlier ranges to smooth them\nsubmission['SalePrice'] = submission['SalePrice'].apply(\n    lambda x: x if x > q1 else x * 0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x\n                                                        if x < q2 else x * 1.1)\nsubmission = submission[['Id', 'SalePrice']]\n\n#saving submission csv\n\nsubmission.to_csv('mysubmission.csv', index=False)\nprint(\n    'Save submission',\n    datetime.now(),\n)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}